{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000 tweet  :   3000\n",
      "3000 label  :   3000\n"
     ]
    }
   ],
   "source": [
    "# data load\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "tweet_list = []\n",
    "label_list = []\n",
    "\n",
    "for index, file in enumerate(glob('data/3000tweet/raw_texts/**/*.txt', recursive=True)): # labels\n",
    "    label_list.append(file.split(\"\\\\\")[1]),\n",
    "    tweet_list.append((open(file, encoding=\"windows-1254\").read().replace('\\n', ' ').strip().lower()))      # tweets\n",
    "\n",
    "print(\"3000 tweet  :  \", len(tweet_list))\n",
    "print(\"3000 label  :  \", len(label_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toplam veri sayısı :  3000\n",
      "1.sınıf :  756\n",
      "2.sınıf :  1287\n",
      "3.sınıf :  957\n"
     ]
    }
   ],
   "source": [
    "print(\"toplam veri sayısı : \",len(label_list))\n",
    "value1 = [i for i in label_list if i in '1']\n",
    "value2 = [i for i in label_list if i in '2']\n",
    "value3 = [i for i in label_list if i in '3']\n",
    "print(\"1.sınıf : \",len(value1))\n",
    "print(\"2.sınıf : \",len(value2))\n",
    "print(\"3.sınıf : \",len(value3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"3000 tweet  :  \", tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "\n",
    "import Stemmer\n",
    "import re\n",
    "\n",
    "\n",
    "def splitIntoStem(message):\n",
    "\n",
    "    \n",
    "    return [removeNumeric(stripEmoji(singleCharacterRemove(removePunctuation\n",
    "                                                           (removeHyperlinks\n",
    "                                                            (removeHashtags\n",
    "                                                             (removeUsernames\n",
    "                                                              (stemWord(word)))))))) for word in message.split()]\n",
    "def stemWord(tweet):\n",
    "\n",
    "    ################################## for turkish\n",
    "    #stemmer = Stemmer.Stemmer('turkish')\n",
    "    #return stemmer.stemWord(tweet).lower()\n",
    "    \n",
    "    return tweet.lower()\n",
    "\n",
    "#remove usernames\n",
    "def removeUsernames(tweet):\n",
    "    return re.sub('@[^\\s]+', '', tweet)\n",
    "\n",
    "\n",
    "#remove hashtag\n",
    "def removeHashtags(tweet):\n",
    "    return re.sub(r'#[^\\s]+', '', tweet)\n",
    "\n",
    "#remove link\n",
    "def removeHyperlinks(tweet):\n",
    "    return re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', '', tweet)\n",
    "\n",
    "#remove numeric character\n",
    "def removeNumeric(value):\n",
    "    blist2 = [item for item in value if not item.isdigit()]\n",
    "    blist3 = \"\".join(blist2)\n",
    "    return blist3\n",
    "\n",
    "#remove punctuation\n",
    "def removePunctuation(tweet):\n",
    "\n",
    "    return re.sub(r'[^\\w\\s]','',tweet)\n",
    "\n",
    "#remove single character\n",
    "def singleCharacterRemove(tweet):\n",
    "    return re.sub(r'(?:^| )\\w(?:$| )', ' ', tweet)\n",
    "\n",
    "#remove emoji\n",
    "def stripEmoji(text):\n",
    "\n",
    "    RE_EMOJI = re.compile('[\\U00010000-\\U0010ffff]', flags=re.UNICODE)\n",
    "    return RE_EMOJI.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dun turkcelle tepkilerimizden sonra bugün turkcell twittera sponsor olmuş. ne tesadüf değil mi ? :)'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "for i,k in enumerate(tweet_list):\n",
    "    tweet_list[i] = \" \".join(splitIntoStem(k)).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['dun',\n",
       " 'turkcelle',\n",
       " 'tepkilerimizden',\n",
       " 'sonra',\n",
       " 'bugün',\n",
       " 'turkcell',\n",
       " 'twittera',\n",
       " 'sponsor',\n",
       " 'olmuş',\n",
       " 'ne',\n",
       " 'tesadüf',\n",
       " 'değil',\n",
       " 'mi']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweet_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tweet_list) :  32588\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def sumTweetWords(tweet_list):\n",
    "    new_value = \"\"\n",
    "    for i in tweet_list:\n",
    "        value = \" \".join(i) \n",
    "        new_value = new_value + \" \" +value\n",
    "    print(\"len(tweet_list) : \",len(new_value.split()))\n",
    "\n",
    "sumTweetWords(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stop_words :  ['acaba', 'ama', 'aslında', 'az', 'bazı', 'belki', 'biri', 'birkaç', 'birşey', 'biz', 'bu', 'çok', 'çünkü', 'da', 'daha', 'de', 'defa', 'diye', 'eğer', 'en', 'gibi', 'hem', 'hep', 'hepsi', 'her', 'hiç', 'için', 'ile', 'ise', 'kez', 'ki', 'kim', 'mı', 'mu', 'mü', 'nasıl', 'ne', 'neden', 'nerde', 'nerede', 'nereye', 'niçin', 'niye', 'o', 'sanki', 'şey', 'siz', 'şu', 'tüm', 've', 'veya', 'ya', 'yani', 'bir', 'iki', 'üç', 'dört', 'beş', 'altı', 'yedi', 'sekiz', 'dokuz', 'on']\n"
     ]
    }
   ],
   "source": [
    "# remove stopwords\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "\n",
    "def removeStopWords(tweet_list):\n",
    "    \n",
    "    filtered_stopwords = []\n",
    "    filtered_stopwords_number = []\n",
    "    \n",
    "    stop_words = stopwords.words('turkish')\n",
    "\n",
    "    stop_words.append(\"bir\")\n",
    "    stop_words.append(\"iki\")\n",
    "    stop_words.append(\"üç\")\n",
    "    stop_words.append(\"dört\")\n",
    "    stop_words.append(\"beş\")\n",
    "    stop_words.append(\"altı\")\n",
    "    stop_words.append(\"yedi\")\n",
    "    stop_words.append(\"sekiz\")\n",
    "    stop_words.append(\"dokuz\")\n",
    "    stop_words.append(\"on\")\n",
    "    \n",
    "    print(\"stop_words : \",stop_words)\n",
    "    \n",
    "    for i in tweet_list:\n",
    "        filtered_sentence = [w for w in i if not w in stop_words]\n",
    "        filtered_stopwords_number.append(filtered_sentence)                      # return list value\n",
    "        filtered_stopwords.append(\" \".join(filtered_sentence))                   # return string value\n",
    "    \n",
    "    return filtered_stopwords,filtered_stopwords_number                                       \n",
    "\n",
    "\n",
    "tweet_list,filtered_stopwords_number = removeStopWords(tweet_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_stopwords_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(tweet_list) :  28919\n"
     ]
    }
   ],
   "source": [
    "sumTweetWords(filtered_stopwords_number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Message Train : 2400 Message Test :  600 Sum Data :  3000\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SVM Clasification: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.52      0.50      0.51       155\n",
      "           2       0.61      0.73      0.67       256\n",
      "           3       0.46      0.35      0.40       189\n",
      "\n",
      "    accuracy                           0.55       600\n",
      "   macro avg       0.53      0.53      0.52       600\n",
      "weighted avg       0.54      0.55      0.54       600\n",
      "\n",
      "SVM Confussion matrix: \n",
      " [[ 77  40  38]\n",
      " [ 28 188  40]\n",
      " [ 44  79  66]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SVM  Standard Accuracy : :  : \n",
      " 0.5516666666666666\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "SVM F1 score: 0.540680767298569\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bayes Clasification: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.24      0.35       155\n",
      "           2       0.51      0.92      0.65       256\n",
      "           3       0.52      0.22      0.31       189\n",
      "\n",
      "    accuracy                           0.52       600\n",
      "   macro avg       0.57      0.46      0.44       600\n",
      "weighted avg       0.56      0.52      0.47       600\n",
      "\n",
      "Bayes Confussion matrix: \n",
      " [[ 37  95  23]\n",
      " [  5 236  15]\n",
      " [ 12 136  41]]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bayes  Standard Accuracy : :  : \n",
      " 0.5233333333333333\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Bayes F1 score: 0.46639100906290026\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report, f1_score, confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score, train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score, confusion_matrix\n",
    "\n",
    "msg_train, msg_test, label_train, label_test = \\\n",
    "    train_test_split(tweet_list,label_list, test_size=0.2)  # random splitting\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Message Train :\", len(msg_train), \"Message Test : \", len(msg_test), \"Sum Data : \",\n",
    "      len(msg_train) + len(msg_test))\n",
    "\n",
    "labels = ['1','2','3']\n",
    "\n",
    "\n",
    "## svm classification\n",
    "\n",
    "svm = Pipeline([('vect', TfidfVectorizer()), ('svm', LinearSVC())])\n",
    "svm.fit(msg_train, label_train)\n",
    "y_pred_class = svm.predict(msg_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print('SVM Clasification: \\n', classification_report(label_test, y_pred_class))  # classification report\n",
    "print('SVM Confussion matrix: \\n', confusion_matrix(label_test, y_pred_class,labels=labels))  # confusion matrix\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"SVM  Standard Accuracy : :  : \\n\", accuracy_score(label_test, y_pred_class))  # accuracy\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print('SVM F1 score:', f1_score(label_test, y_pred_class, average='weighted'))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "## Bayes classification\n",
    "\n",
    "bayes = Pipeline([('vect', TfidfVectorizer()), ('bayes', MultinomialNB())])\n",
    "bayes.fit(msg_train, label_train)\n",
    "y_pred_class = bayes.predict(msg_test)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print('Bayes Clasification: \\n', classification_report(label_test, y_pred_class))  # classification report\n",
    "print('Bayes Confussion matrix: \\n', confusion_matrix(label_test, y_pred_class,labels=labels))  # confusion matrix\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print(\"Bayes  Standard Accuracy : :  : \\n\", accuracy_score(label_test, y_pred_class))  # accuracy\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "print('Bayes F1 score:', f1_score(label_test, y_pred_class, average='weighted'))\n",
    "print(\"\\n\")\n",
    "print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1']\n"
     ]
    }
   ],
   "source": [
    "# sample test data predict\n",
    "\n",
    "msg_test2=[\"süperonline kullandıkça  mutlu oluyorum \"]\n",
    "y_pred_class = svm.predict(msg_test2)\n",
    "print(y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Cross Validation (Tf-idf): \n",
      " [0.6        0.54       0.59666667 0.54333333 0.57666667 0.53666667\n",
      " 0.6        0.54666667 0.55666667 0.54      ]\n",
      "SVM Accuracy (Tf-idf): 0.56 (+/- 0.05) \n",
      "\n",
      "\n",
      "SVM Cross Validation (Tf): \n",
      " [0.61       0.53666667 0.59333333 0.54       0.56333333 0.56333333\n",
      " 0.58333333 0.52666667 0.52       0.55666667]\n",
      "SVM Accuracy (Tf): 0.56 (+/- 0.06) \n",
      "\n",
      "\n",
      "Bayes Cross Validation (Tf-idf): \n",
      " [0.59333333 0.55333333 0.56666667 0.55333333 0.56333333 0.52666667\n",
      " 0.54333333 0.54666667 0.53666667 0.51333333]\n",
      "Bayes Accuracy (Tf-idf): 0.55 (+/- 0.04) \n",
      "\n",
      "\n",
      "Bayes Cross Validation (Tf) : \n",
      " [0.61       0.54666667 0.58333333 0.53666667 0.57333333 0.54\n",
      " 0.58333333 0.57666667 0.54333333 0.54      ]\n",
      "Bayes Accuracy (Tf): 0.56 (+/- 0.05)\n"
     ]
    }
   ],
   "source": [
    "## Cross Validation\n",
    "\n",
    "\n",
    "svmtfidf = Pipeline([('vect', TfidfVectorizer()), ('svm', LinearSVC())])\n",
    "scorestfidf = cross_val_score(svmtfidf, tweet_list, label_list, cv=10)  # cross validation k=10\n",
    "\n",
    "print(\"SVM Cross Validation (Tf-idf): \\n\", scorestfidf)\n",
    "print(\"SVM Accuracy (Tf-idf): %0.2f (+/- %0.2f)\" % (scorestfidf.mean(), scorestfidf.std() * 2), \"\\n\\n\")\n",
    "\n",
    "svmtf = Pipeline([('vect', CountVectorizer()), ('svm', LinearSVC())])\n",
    "scorestf = cross_val_score(svmtf, tweet_list, label_list, cv=10)  # cross validation k=10\n",
    "\n",
    "print(\"SVM Cross Validation (Tf): \\n\", scorestf)\n",
    "print(\"SVM Accuracy (Tf): %0.2f (+/- %0.2f)\" % (scorestf.mean(), scorestf.std() * 2), \"\\n\\n\")\n",
    "\n",
    "bayestfidf = Pipeline([('vect', TfidfVectorizer()), ('bayes', MultinomialNB())])\n",
    "scorestfidf = cross_val_score(bayestfidf, tweet_list, label_list, cv=10)\n",
    "\n",
    "print(\"Bayes Cross Validation (Tf-idf): \\n\", scorestfidf)\n",
    "print(\"Bayes Accuracy (Tf-idf): %0.2f (+/- %0.2f)\" % (scorestfidf.mean(), scorestfidf.std() * 2), \"\\n\\n\")\n",
    "\n",
    "bayestf = Pipeline([('vect', CountVectorizer()), ('bayes', MultinomialNB())])\n",
    "scorestf = cross_val_score(bayestf, tweet_list, label_list, cv=10)\n",
    "\n",
    "print(\"Bayes Cross Validation (Tf) : \\n\", scorestf)\n",
    "print(\"Bayes Accuracy (Tf): %0.2f (+/- %0.2f)\" % (scorestf.mean(), scorestf.std() * 2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
